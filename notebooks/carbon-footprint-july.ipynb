{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70e846a",
   "metadata": {},
   "source": [
    "# Calculating Carbon Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6e87d",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we will expand on the previous notebooks, creating functions that query both Victoria Metrics (in order to obtain node power readings) and a public carbon intensity API (in order to obtain the carbon intensity in the UK for the given times). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32103f24",
   "metadata": {},
   "source": [
    "## Preparing the Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8bb58",
   "metadata": {},
   "source": [
    "We will first import all of the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07397f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ffd54",
   "metadata": {},
   "source": [
    "We will now set up our jupyter notebook to make queries from VictoriaMetrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have removed the code to set up the jupyter notebook to make queries from VictoriaMetrics since it was accessing a private database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e83e9a",
   "metadata": {},
   "source": [
    "## Loading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f2c29",
   "metadata": {},
   "source": [
    "We have saved the processed Slurm data in csv files, which we will now load into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b79939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now read the .csv file containing the Slurm data for June into a DataFrame\n",
    "sSlurmDataPath = '../data/dfSacctFinal.csv'\n",
    "dfSacct = pd.read_csv(sSlurmDataPath, index_col=0, parse_dates=['Start', 'End'], infer_datetime_format=True)\n",
    "\n",
    "# We will now read the .csv file containing the extended Slurm data for June into a DataFrame. \n",
    "sSlurmExtendedPath = '../data/dfSacctExtended.csv'\n",
    "dfSacctExtended = pd.read_csv(sSlurmExtendedPath, index_col=0, parse_dates=['Start', 'End'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a205e",
   "metadata": {},
   "source": [
    "## Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c742c05",
   "metadata": {},
   "source": [
    "We will start by creating all of the necessary functions before applying then to a set of test data to ensure that the code works as expected. \n",
    "\n",
    "Once we know that the code works, we will apply the functions to the job data above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba1639",
   "metadata": {},
   "source": [
    "## Ensuring the Time is in UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6bdb0",
   "metadata": {},
   "source": [
    "We must first ensure that all times are in UTC so that we access the correct power readings from Victoria Metrics. \n",
    "\n",
    "Therefore, we will create a function that converts all times in the DataFrame to UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3237c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfToUTC(df):\n",
    "    \"\"\"\n",
    "    Returns a pd DataFrame containing columns for the start and end times in UTC.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pdDataFrame \n",
    "        The pd DataFrame containing all of the job data. This DataFrame must contain \n",
    "        a 'Start' and 'End' column of pd DateTime64 objects. \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    df: pdDataFrame\n",
    "        The pd DataFrame that was passed in as a parameter with two new columns:\n",
    "        'StartUTC' and 'EndUTC' of pd DateTime64 objects, which contain the original \n",
    "        start and end times in UTC rather than local time. \n",
    "    \"\"\"\n",
    "\n",
    "    df['UTCStart'] = df['Start'].dt.tz_localize('Europe/London')\n",
    "    df['UTCEnd'] = df['End'].dt.tz_localize('Europe/London')\n",
    "\n",
    "    df['UTCStart'] = df['UTCStart'].dt.tz_convert(pytz.utc)\n",
    "    df['UTCEnd'] = df['UTCEnd'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e0407",
   "metadata": {},
   "source": [
    "## Querying Victoria Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f7666",
   "metadata": {},
   "source": [
    "Rather than constantly querying Victoria Metrics, I thought it would be a good idea to  query Victoria Metrics once at the start and store all of the power data for all of the nodes across the entire July time period in a .csv file called *VMPowerJuly.csv*.\n",
    "\n",
    "Below I created a function that checks whether the file *VMPowerJuly.csv* exists. If it exists, the function will return a DataFrame containing the data in the .csv file. If it does not exist, the function will query Victoria Metrics and will create the file *VMPowerJuly.csv* to store the data. The function will then return a DataFrame containig the power data. \n",
    "\n",
    "*NOTE: We will not be using this function for our program. The function below uses a lot of RAM because pandas stores DataFrames in memory rather than on the disk and is too inefficient to use on all of the data. As a result we will query Victoria Metrics for each job rather than making one big query at the start.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPowerDataMonth(dfJobs):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing the Victoria Metrics power data for the specified month.\n",
    "\n",
    "    Checks whether the .csv file containing the Victoria Metrics power data for the specified \n",
    "    month (with the format 'VMPower<Month>.csv') exists. If the file exists the DataFrame \n",
    "    containing the power data is returned. If the file does not exist, Victoria Metrics is \n",
    "    queried and the .csv file is created; the function then returns the DataFrame containing\n",
    "    the power data. If there is a problem while querying Victoria Metrics, the function will \n",
    "    return None.\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    dfJobs: pdDataFrame\n",
    "        The DataFrame containing all of the job data that is to be processed. This DataFrame \n",
    "        must have already been processed to contain UTCStart and UTCEnd columns. This can be \n",
    "        done using the dfToUTC() function. \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dfJobPower: pdDataFrame\n",
    "        The DataFrame containing all of the power data. \n",
    "    None: NoneType\n",
    "        Returned if there is a problem while querying Victoria Metrics.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # We start by obtaining the file name from the input DataFrame\n",
    "    sMonth = dfJobs['UTCStart'].dt.month_name(locale='English').value_counts().index[0]\n",
    "    sFileName = '../data/VMPower' + sMonth + '.csv'\n",
    "\n",
    "    # We will now open the file and check whether it is empty. If the file is \n",
    "    # empty we will query Victoria Metrics and obtain the data. If the file is \n",
    "    # not empty we will load in the data and return a DataFrame. \n",
    "    fPowerData = open(sFileName, 'a+')\n",
    "\n",
    "    fPowerData.seek(0)\n",
    "    bEmpty = len(fPowerData.read()) == 0\n",
    "\n",
    "    fPowerData.close()\n",
    "\n",
    "    if bEmpty:\n",
    "        # We will now obtain the start and end dates and times for our query.\n",
    "        start = dfJobs.iloc[0]['UTCStart'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        end = dfJobs.iloc[round(len(dfJobs)/30)]['UTCEnd'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        # We will now query Victoria Metrics to obtain the power data\n",
    "        data = {\n",
    "            'query': f'amperageProbeReading{{amperageProbeLocationName=\"System Board Pwr Consumption\"}}',\n",
    "            'start': start,\n",
    "            'end' : end,\n",
    "            'step': '30s'\n",
    "        }\n",
    "\n",
    "        response = requests.put(\n",
    "            url, \n",
    "            data=data,\n",
    "            proxies=proxies,\n",
    "            headers=headers,\n",
    "            timeout=10\n",
    "        )  \n",
    "\n",
    "        # We will now ensure that the request was successfull \n",
    "        if (response.status_code != 200):\n",
    "            print('Response status code was not 200.')\n",
    "            print(f'The response was {response.status_code}')\n",
    "            return None\n",
    "\n",
    "        # We will now create a DataFrame containing the power data\n",
    "        dNodePowers = {}\n",
    "        dPowerData = {}\n",
    "        lTicks = []\n",
    "        lNodes = []\n",
    "\n",
    "        for dNodeData in response.json()['data']['result']:\n",
    "\n",
    "            # Here we remove any entry's that do not have an alias.\n",
    "            if 'alias' not in dNodeData['metric'].keys():\n",
    "                continue \n",
    "\n",
    "            sNode = dNodeData['metric']['alias']\n",
    "            lData = dNodeData['values']\n",
    "\n",
    "            lNodes.append(sNode)\n",
    "\n",
    "            dNodePowers[sNode] = lData\n",
    "\n",
    "            for lDataPoint in lData:\n",
    "                iTick = lDataPoint[0]\n",
    "                iPower = lDataPoint[1]\n",
    "                lTicks.append(iTick)\n",
    "                dPowerData[(iTick, sNode)] = iPower\n",
    "        \n",
    "        lTicks.sort()\n",
    "        setTicksOrdered = set(lTicks)\n",
    "\n",
    "        # Below we create the structure of the DataFrame\n",
    "        dfJobPower = pd.DataFrame(\n",
    "            index = setTicksOrdered,\n",
    "            columns = lNodes\n",
    "        )\n",
    "\n",
    "        # We now populate the DataFrame with the power data. \n",
    "        for tIndex in dPowerData.keys():\n",
    "            dfJobPower.loc[tIndex[0], tIndex[1]] = dPowerData[tIndex]\n",
    "\n",
    "        # We now change the format of the timestamp\n",
    "        dfJobPower.index = pd.to_datetime(dfJobPower.index, unit='s', utc=True)\n",
    "        dfJobPower['Date'] = dfJobPower.index.strftime('%Y-%m-%d')\n",
    "        dfJobPower['Date'] = dfJobPower['Date'].str.cat((((dfJobPower.index).hour * 2) + ((dfJobPower.index).minute//30) + 1).astype(str), sep=\" \")\n",
    "\n",
    "        # We now resample the DataFrame and interpolate the data to obtain \n",
    "        # a datapoint for each 30s. \n",
    "        dfJobPower[dfJobPower.columns[:-1]] = dfJobPower[dfJobPower.columns[:-1]].apply(pd.to_numeric, axis=1)\n",
    "        dfJobPower = dfJobPower.resample('30S').interpolate()\n",
    "\n",
    "        # We now save the DataFrame as a .csv file and return the DataFrame\n",
    "        dfJobPower.to_csv(sFileName)\n",
    "\n",
    "        return dfJobPower\n",
    "    \n",
    "    # Below we read in the .csv file containing the power data\n",
    "    # and we format the DataFrame before returning it. \n",
    "    dfJobPower = pd.read_csv(sFileName, parse_dates=[0], infer_datetime_format=True)\n",
    "    dfJobPower.set_index('Unnamed: 0', inplace=True)\n",
    "    dfJobPower.index.rename('Timestamp', inplace=True)\n",
    "\n",
    "    return dfJobPower\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125e7e0",
   "metadata": {},
   "source": [
    "## Calculating the Energy Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04708966",
   "metadata": {},
   "source": [
    "We will now write code to calculate the energy consumption of each job, in Wh, for each 30 minute time period that the job runs. \n",
    "\n",
    "*NOTE: We separate the energy consumption into 30 minute time periods because the carbon API we are accessing provides a separate carbon intensity for each 30 minute period of the day.*\n",
    "\n",
    "We will first create a function that queries Victoria Metrics and returns a DataFrame of the power usage for each node the job runs on for the duration of the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad270994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobPower(jobID, dfJobs):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing the power readings, in W, on each node that the job runs on for the duration of the job. \n",
    "\n",
    "    Returns a DataFrame whose index is the timestamp and whose columns are the power readings, in W, for each node the job \n",
    "    runs on. If there is a problem while querying victoria metrics, an exception is thrown. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jobID: integer\n",
    "        The integer job ID for the job in question. \n",
    "    dfJobs: pdDataFrame\n",
    "        The DataFrame containing all of the job data for the time period in question. \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dfJobPower: pdDataFrame\n",
    "        The DataFrame containing the power readings, in W, for each node the job runs on. \n",
    "    None: NoneType\n",
    "        Returns None if there is a problem while querying Victoria Metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # We must first find the UTC start and end time of the job. \n",
    "    # We need to reformat these times to the correct format for Victoria Metrics. \n",
    "    if len(dfJobs.loc[jobID].to_frame().transpose()) > 1:\n",
    "        sStart = dfJobs.loc[jobID, 'UTCStart'].iloc[0].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        sEnd = dfJobs.loc[jobID, 'UTCEnd'].iloc[0].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    else: \n",
    "        sStart = dfJobs.loc[jobID, 'UTCStart'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        sEnd = dfJobs.loc[jobID, 'UTCEnd'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # We will now create a list of the nodes that the job runs on.\n",
    "    if sum(dfJobs.index == jobID) > 1:\n",
    "        lNodeList = list(dfJobs.loc[jobID, 'NodeList'])\n",
    "    else:\n",
    "        lNodeList = [dfJobs.loc[jobID, 'NodeList']]\n",
    "\n",
    "    # We will now create the string containing the logical node query for Victoria Metrics\n",
    "    sNodeQuery = '|'.join(lNodeList)\n",
    "\n",
    "    # We will now query Victoria Metrics to obtain the power data\n",
    "    data = {\n",
    "        'query': f'amperageProbeReading{{alias=~\"{sNodeQuery}\", amperageProbeLocationName=\"System Board Pwr Consumption\"}}',\n",
    "        'start': sStart,\n",
    "        'end' : sEnd,\n",
    "        'step': '30s'\n",
    "    }\n",
    "\n",
    "    response = requests.put(\n",
    "        url, \n",
    "        data=data,\n",
    "        proxies=proxies,\n",
    "        headers=headers,\n",
    "        timeout=10\n",
    "    )  \n",
    "\n",
    "    # We will now ensure that the request was successfull \n",
    "    if (response.status_code != 200):\n",
    "        print('Response status code was not 200.')\n",
    "        print(f'The response was {response.status_code}')\n",
    "        return None\n",
    "\n",
    "    # We will now create a DataFrame containing the power data\n",
    "    dNodePowers = {}\n",
    "    dPowerData = {}\n",
    "    lTicks = []\n",
    "    lNodes = []\n",
    "\n",
    "    for dNodeData in response.json()['data']['result']:\n",
    "        sNode = dNodeData['metric']['alias']\n",
    "        \n",
    "        if sNode in lNodes:\n",
    "            continue \n",
    "\n",
    "        lData = dNodeData['values']\n",
    "        lNodes.append(sNode)\n",
    "        dNodePowers[sNode] = lData\n",
    "\n",
    "        for lDataPoint in lData:\n",
    "            iTick = lDataPoint[0]\n",
    "            iPower = lDataPoint[1]\n",
    "            lTicks.append(iTick)\n",
    "            dPowerData[(iTick, sNode)] = iPower\n",
    "    \n",
    "    lTicks.sort()\n",
    "    setTicksOrdered = set(lTicks)\n",
    "\n",
    "    # Below we create the structure of the DataFrame\n",
    "    dfJobPower = pd.DataFrame(\n",
    "        index = setTicksOrdered,\n",
    "        columns = lNodes\n",
    "    )\n",
    "\n",
    "    # We now populate the DataFrame with the power data. \n",
    "    for tIndex in dPowerData.keys():\n",
    "        dfJobPower.loc[tIndex[0], tIndex[1]] = dPowerData[tIndex]\n",
    "\n",
    "    # We now change the format of the timestamp\n",
    "    dfJobPower.index = pd.to_datetime(dfJobPower.index, unit='s', utc=True)\n",
    "    dfJobPower['Date'] = dfJobPower.index.strftime('%Y-%m-%d')\n",
    "    dfJobPower['Date'] = dfJobPower['Date'].str.cat((((dfJobPower.index).hour * 2) + ((dfJobPower.index).minute//30) + 1).astype(str), sep=\" \")\n",
    "\n",
    "    # We now resample the DataFrame and interpolate the data to obtain \n",
    "    # a datapoint for each 30s. \n",
    "    dfJobPower[dfJobPower.columns[:-1]] = dfJobPower[dfJobPower.columns[:-1]].apply(pd.to_numeric, axis=1)\n",
    "    dfJobPower = dfJobPower.resample('30S').interpolate()\n",
    "\n",
    "    return dfJobPower\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c965bb",
   "metadata": {},
   "source": [
    "We will now create a function that calculates the energy consumed by each node the job runs on for each 30 minute period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb12b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobEnergy(dfPowerData):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing the energy consumed by each node the job runs on (in Wh) for each 30 minute time period of the Job's duration. \n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    dfPowerData: pdDataFrame\n",
    "        The DataFrame containin the Victoria Metrics power data for each node the job runs on for the job's running period. \n",
    "        This DataFrame is returned by the getJobPower() function.  \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dfNodeEnergies: pdDataFrame\n",
    "        The DataFrame containing the energy consumed by each node (in Wh) for each 30 minute time period. \n",
    "    \"\"\"\n",
    "\n",
    "    dEnergies = {}\n",
    "\n",
    "    # Below we iterate through each node, creating a dictionary of energies for \n",
    "    # each 30 minute interval.\n",
    "    for sNode in dfPowerData.columns[:-1]:\n",
    "            dIntervalEnergies = {}\n",
    "\n",
    "            # Below iterate through each 30 minute interval, calculating the \n",
    "            # energy for that interval.\n",
    "\n",
    "            # We will first create a list of DataFrames, one for each interval in the \n",
    "            # time frame. \n",
    "            lPeriodDFs = []\n",
    "\n",
    "            for interval in dfPowerData['Date'].unique():                \n",
    "                bIntervalMask = dfPowerData['Date'] == interval\n",
    "                lPeriodDFs.append(dfPowerData[bIntervalMask])\n",
    "\n",
    "            for dfIndex in range(len(lPeriodDFs)):\n",
    "                interval = lPeriodDFs[dfIndex]['Date'].unique()[0]\n",
    "\n",
    "                if dfIndex != 0:\n",
    "                        dfIntervalPower = pd.concat([lPeriodDFs[dfIndex - 1].iloc[-1].to_frame().transpose(), lPeriodDFs[dfIndex]])\n",
    "                else:\n",
    "                        dfIntervalPower = lPeriodDFs[dfIndex]\n",
    "\n",
    "                if dfIntervalPower[sNode].isnull().values.any():\n",
    "                        dIntervalEnergies[interval] = None\n",
    "                        continue \n",
    "\n",
    "                iJoules = np.trapz(dfIntervalPower[sNode].astype(int), dx=30)\n",
    "                iWattHour = iJoules/3600\n",
    "\n",
    "                dIntervalEnergies[interval] = iWattHour\n",
    "\n",
    "            dEnergies[sNode] = dIntervalEnergies\n",
    "\n",
    "    # Below we create a DataFrame from the dictionary of energies.\n",
    "    dfNodeEnergies = pd.DataFrame.from_dict(dEnergies)\n",
    "    \n",
    "    return dfNodeEnergies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6a2ca",
   "metadata": {},
   "source": [
    "## Querying the Carbon Intensity API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d89bbf",
   "metadata": {},
   "source": [
    "As we do not want to contnuously query the public carbon intensity API, we are now going to create a function that queries the API to obtain all of the carbon intensity data for the entire time period we are interested in. We will then store this carbon intensity data in a .csv file (if it doesn't already exist).\n",
    "\n",
    "*NOTE: The API only allows for a maximum date range of 30 days*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49310551",
   "metadata": {},
   "source": [
    "We will now create a function that returns a DataFrame of all the carbon intensities that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee1edeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCarbonIntensities(dfJobs):\n",
    "    \"\"\" \n",
    "    Returns a DataFrame of the carbon intensities for each 30 minute time period in the interval.\n",
    "\n",
    "    Checks whether the .csv file containing the carbon intensities for the given month, with the \n",
    "    format 'CarbonIntensities<Month>.csv' already exists. If it exists, returns a DataFrame, from \n",
    "    the .csv file, whose index is the time period, in the format 'YYYY-MM-DD PERIOD' where PERIOD \n",
    "    is the 30 minute time period of that date as an integer from 1-48. The DataFrame contains all \n",
    "    carbon intensities, in gCO2/kWh, for each 30 minute time period in the given interval. If the \n",
    "    .csv file does not exist, the DataFrame with the format above will be created and saved into a\n",
    "    .csv file, before being returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfJobs: pdDataFrame\n",
    "        The DataFrame containing all of the job data for the given time period. \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dfIntensities: pdDataFrame \n",
    "        A DataFrame containing the carbon intensity, in gCO2/kWh, for each 30 minute interval within\n",
    "        the specified time range. \n",
    "    None: NoneType\n",
    "        Returns 'None' if there is a problem accessing the carbon intensity API.\n",
    "    \"\"\"\n",
    "\n",
    "    # We start by obtaining the file name from the input DataFrame\n",
    "    sMonth = dfJobs['UTCStart'].dt.month_name(locale='English').value_counts().index[0]\n",
    "    sFileName = '../data/CarbonIntensities' + sMonth + '.csv'\n",
    "\n",
    "    # We will now open the file and check whether it is empty. If the file is \n",
    "    # empty we will create the carbon intensity DataFrame. If the file is not\n",
    "    # empty we will load in the data and return a DataFrame. \n",
    "    fCarbonData = open(sFileName, 'a+')\n",
    "\n",
    "    fCarbonData.seek(0)\n",
    "    bEmpty = len(fCarbonData.read()) == 0\n",
    "\n",
    "    fCarbonData.close()\n",
    "\n",
    "    if bEmpty:\n",
    "        # We will first find the start and end dates and times for the period \n",
    "        # over which the jobs run. \n",
    "        start = dfJobs.iloc[0]['UTCStart']\n",
    "        end = dfJobs.iloc[-1]['UTCEnd']\n",
    "\n",
    "        # We must first check if the date range is longer than 30 days. \n",
    "        # If it is, we will split the date range into chunks that are 30\n",
    "        # days or shorter. \n",
    "        timeDeltaSeconds = (end-start).total_seconds()\n",
    "        iChunks = int(np.ceil(timeDeltaSeconds/(30 * 86400)))\n",
    "\n",
    "        lCarbonDFs = []\n",
    "\n",
    "        for iCount in range(iChunks):\n",
    "            start = start + pd.Timedelta((30 * iCount), 'd')\n",
    "            if iCount < iChunks - 1:\n",
    "                tempEnd = start + pd.Timedelta(30, 'd')\n",
    "            else:\n",
    "                #tempEnd = end + pd.Timedelta(30, 'min')\n",
    "                tempEnd = end.date() + pd.Timedelta(24, 'h')\n",
    "            \n",
    "            sStart = start.strftime(\"%Y-%m-%dT%H:%MZ\")\n",
    "            sEnd = tempEnd.strftime(\"%Y-%m-%dT%H:%MZ\")\n",
    "\n",
    "            # We will now request the carbon intensity data from the API\n",
    "            # and we will check if the request was successful. If the request\n",
    "            # is unsuccessful we will return None. \n",
    "            intensity = requests.get(f'https://api.carbonintensity.org.uk/intensity/{sStart}/{sEnd}')\n",
    "\n",
    "            if (intensity.status_code == 400):\n",
    "                print('Status code was 400.')\n",
    "                print('There was a bad request.')\n",
    "                return None\n",
    "            elif (intensity.status_code == 500):\n",
    "                print('Status code was 500.')\n",
    "                print('There was an internal server error.')\n",
    "                return None\n",
    "\n",
    "            # We will now create and reformat a DataFrame from the \n",
    "            # carbon intensity data. \n",
    "            dfTempIntensities = pd.DataFrame(intensity.json()['data'])\n",
    "\n",
    "            dfTempIntensities['from'] = pd.to_datetime(dfTempIntensities['from'])\n",
    "            dfTempIntensities['date'] = dfTempIntensities['from'].dt.date\n",
    "            dfTempIntensities['period'] = dfTempIntensities['date'].astype(str) + \" \" + ((dfTempIntensities['from'].dt.hour * 2) + (dfTempIntensities['from'].dt.minute//30) + 1).astype(str)\n",
    "\n",
    "            dfTempIntensities.drop(columns=['from', 'to', 'date'], inplace=True)\n",
    "            dfTempIntensities['intensity'] = pd.json_normalize(dfTempIntensities['intensity'])['actual']\n",
    "            dfTempIntensities.set_index('period', inplace=True)\n",
    "\n",
    "            lCarbonDFs.append(dfTempIntensities)\n",
    "\n",
    "        dfIntensities = pd.concat(lCarbonDFs)\n",
    "\n",
    "        # We will now save the DataFrame to a .csv file and return it. \n",
    "        dfIntensities.to_csv(sFileName)\n",
    "        \n",
    "        return dfIntensities\n",
    "\n",
    "    # Below we will read in the data from the .csv file to a DataFrame \n",
    "    # which we will return.\n",
    "    dfIntensities = pd.read_csv(sFileName, parse_dates=[0], infer_datetime_format=True)\n",
    "    dfIntensities.set_index('period', inplace=True)\n",
    "\n",
    "    return dfIntensities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c98a10",
   "metadata": {},
   "source": [
    "## Calculating the Carbon Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e261731",
   "metadata": {},
   "source": [
    "We will now write a function to calculate the carbon footprint given the Watt Hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f3b3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCarbonFootprint(jobID, df):\n",
    "     \"\"\"\n",
    "    Returns a DataFrame containing the job's carbon footprint data.\n",
    "\n",
    "    Returns a DataFrame containing the job's energy consumption, in Wh; \n",
    "    carbon footprint, in gCO2; and the distance driven by a medium sized\n",
    "    diesel car, in km, that releases the same amount of carbon dioxide. \n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    jobID: integer\n",
    "        The integer job ID of the job whose carbon footprint is calculated.\n",
    "    df: pdDataFrame\n",
    "        The pandas DataFrame containing all the job data.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dfCarbonData: pd.DataFrame\n",
    "        The pd DataFrame containing the job's carbon data.  \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dfJobPower = getJobPower(jobID, df)\n",
    "\n",
    "    if isinstance(dfJobPower, pd.DataFrame) and dfJobPower.isnull().values.any():\n",
    "        print(f'Missing Data for job: {jobID}')\n",
    "        return pd.DataFrame([np.nan, np.nan]) \n",
    "    elif isinstance(dfJobPower, type(None)):\n",
    "        print(f'Missing Data for job: {jobID}')\n",
    "        return pd.DataFrame([np.nan, np.nan]) \n",
    "    \n",
    "    \n",
    "    dfJobEnergy = getJobEnergy(dfJobPower)\n",
    "\n",
    "    if '2023-07-27 16' in dfJobEnergy.index:\n",
    "        print(jobID)\n",
    "\n",
    "    dfJobCarbonIntensities = getCarbonIntensities(df).loc[dfJobEnergy.index]\n",
    "\n",
    "    dfJobEnergy.rename(columns={dfJobEnergy.columns[0] : 'Data'}, inplace=True)\n",
    "    dfJobCarbonIntensities.rename(columns={dfJobCarbonIntensities.columns[0] : 'Data'}, inplace=True)\n",
    "\n",
    "    iEnergyTotal = sum(dfJobEnergy[dfJobEnergy.columns[0]])\n",
    "\n",
    "    dfJobCarbon = (dfJobEnergy * dfJobCarbonIntensities)/1000\n",
    "\n",
    "    iCarbon = round(sum(dfJobCarbon['Data']))\n",
    "\n",
    "    iDistance = iCarbon/171\n",
    "\n",
    "    return pd.DataFrame([iCarbon, iEnergyTotal, iDistance])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca42e77",
   "metadata": {},
   "source": [
    "## Testing the Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347fd0b",
   "metadata": {},
   "source": [
    "In order to test the carbon footprint code above, I am going to create some test power reading data which will have the following structure: \n",
    "\n",
    "    - A power of 300 W for the first 1800 seconds.\n",
    "    - A power of 150 W for the next 1289 seconds. \n",
    "    - A power of 300 W for the next 1800 seconds.\n",
    "\n",
    "The total energy consumed by this test job should be 353.7 wh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc522163",
   "metadata": {},
   "source": [
    "I will now create a test job DataFrame containing the test job only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab0338e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Temp\\ipykernel_28780\\1080419065.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dfTest = dfSacctExtended[dfSacctExtended.index.value_counts() == 1].iloc[-1].to_frame().transpose().reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "dfTest = dfSacctExtended[dfSacctExtended.index.value_counts() == 1].iloc[-1].to_frame().transpose().reset_index(drop=True)\n",
    "\n",
    "dfTest['End'] = pd.to_datetime(\"2023-07-27 09:35:05\")\n",
    "dfTest['UTCEnd'] = pd.to_datetime(\"2023-07-27 08:35:05+00:00\")\n",
    "dfTest['NodeList'] = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f081b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobName</th>\n",
       "      <th>Partition</th>\n",
       "      <th>ElapsedRaw</th>\n",
       "      <th>Account</th>\n",
       "      <th>State</th>\n",
       "      <th>NodeList</th>\n",
       "      <th>User</th>\n",
       "      <th>QOS</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Timelimit</th>\n",
       "      <th>Suspended</th>\n",
       "      <th>ExclusiveCPU</th>\n",
       "      <th>ExclusiveOverlapping</th>\n",
       "      <th>Exclusive</th>\n",
       "      <th>SharedSameUser</th>\n",
       "      <th>UTCEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171f8e926c82dd83fc75053ec9b110f092aa32b41d5c98...</td>\n",
       "      <td>ampere</td>\n",
       "      <td>29</td>\n",
       "      <td>99fdc1f587a9423c1abc5a1ce22053628b94a5226d3c0f...</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>Test</td>\n",
       "      <td>dd68b7c728069b005e5dac9c3e9d59a7379b1347fa4e6f...</td>\n",
       "      <td>gpu2</td>\n",
       "      <td>2023-07-27 08:13:36</td>\n",
       "      <td>2023-07-27 09:35:05</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-07-27 08:35:05+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             JobName Partition ElapsedRaw  \\\n",
       "0  171f8e926c82dd83fc75053ec9b110f092aa32b41d5c98...    ampere         29   \n",
       "\n",
       "                                             Account      State NodeList  \\\n",
       "0  99fdc1f587a9423c1abc5a1ce22053628b94a5226d3c0f...  COMPLETED     Test   \n",
       "\n",
       "                                                User   QOS  \\\n",
       "0  dd68b7c728069b005e5dac9c3e9d59a7379b1347fa4e6f...  gpu2   \n",
       "\n",
       "                Start                 End Timelimit Suspended ExclusiveCPU  \\\n",
       "0 2023-07-27 08:13:36 2023-07-27 09:35:05  02:00:00  00:00:00        False   \n",
       "\n",
       "  ExclusiveOverlapping Exclusive SharedSameUser                    UTCEnd  \n",
       "0                False     False           True 2023-07-27 08:35:05+00:00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690690e0",
   "metadata": {},
   "source": [
    "The test power data will be created using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3278d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lPowerTest = []\n",
    "\n",
    "for tTick in list(range(1690355610, 1690360500, 30)):\n",
    "    if tTick < (1690355610 + 1800):\n",
    "        lPowerTest.append([tTick, 300])\n",
    "    elif (1690355610 + 1800) < tTick and tTick < (1690355610 + 3089):\n",
    "        lPowerTest.append([tTick, 150])\n",
    "    elif tTick > (1690355610 + 3089):\n",
    "        lPowerTest.append([tTick, 300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e104089",
   "metadata": {},
   "source": [
    "I am now going to modify the getJobPower() function to use out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a948bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobPower(jobID, dfJobs):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing the power readings, in W, on each node that the job runs on for the duration of the job. \n",
    "\n",
    "    Returns a DataFrame whose index is the timestamp and whose columns are the power readings, in W, for each node the job \n",
    "    runs on. If there is a problem while querying victoria metrics, an exception is thrown. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jobID: integer\n",
    "        The integer job ID for the job in question. \n",
    "    dfJobs: pdDataFrame\n",
    "        The DataFrame containing all of the job data for the time period in question. \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dfJobPower: pdDataFrame\n",
    "        The DataFrame containing the power readings, in W, for each node the job runs on. \n",
    "    None: NoneType\n",
    "        Returns None if there is a problem while querying Victoria Metrics\n",
    "    \"\"\"\n",
    "\n",
    "    if jobID != 0:\n",
    "        # We must first find the UTC start and end time of the job. \n",
    "        # We need to reformat these times to the correct format for Victoria Metrics. \n",
    "        if dfJobs.index.value_counts().loc[jobID] > 1:\n",
    "            sStart = dfJobs.loc[jobID, 'UTCStart'].iloc[0].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            sEnd = dfJobs.loc[jobID, 'UTCEnd'].iloc[0].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        else: \n",
    "            sStart = dfJobs.loc[jobID, 'UTCStart'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            sEnd = dfJobs.loc[jobID, 'UTCEnd'].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        # We will now create a list of the nodes that the job runs on.\n",
    "        if sum(dfJobs.index == jobID) > 1:\n",
    "            lNodeList = list(dfJobs.loc[jobID, 'NodeList'])\n",
    "        else:\n",
    "            lNodeList = [dfJobs.loc[jobID, 'NodeList']]\n",
    "\n",
    "        # We will now create the string containing the logical node query for Victoria Metrics\n",
    "        sNodeQuery = '|'.join(lNodeList)\n",
    "\n",
    "        # We will now query Victoria Metrics to obtain the power data\n",
    "        data = {\n",
    "            'query': f'amperageProbeReading{{alias=~\"{sNodeQuery}\", amperageProbeLocationName=\"System Board Pwr Consumption\"}}',\n",
    "            'start': sStart,\n",
    "            'end' : sEnd,\n",
    "            'step': '30s'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.put(\n",
    "            url, \n",
    "            data=data,\n",
    "            proxies=proxies,\n",
    "            headers=headers,\n",
    "            timeout=10\n",
    "        )  \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print('ConnecionError')\n",
    "            return None\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print('ReadRimeout')\n",
    "            return None\n",
    "\n",
    "        # We will now ensure that the request was successfull \n",
    "        if (response.status_code != 200):\n",
    "            print('Response status code was not 200.')\n",
    "            print(f'The response was {response.status_code}')\n",
    "            return None\n",
    "\n",
    "        # We will now check that the result is not empty.\n",
    "        if len(response.json()['data']['result']) == 0:\n",
    "            print('No data.')\n",
    "            return None\n",
    "\n",
    "        # We will now create a DataFrame containing the power data\n",
    "        dNodePowers = {}\n",
    "        dPowerData = {}\n",
    "        lTicks = []\n",
    "        lNodes = []\n",
    "\n",
    "        for dNodeData in response.json()['data']['result']:\n",
    "            sNode = dNodeData['metric']['alias']\n",
    "            \n",
    "            if sNode in lNodes:\n",
    "                continue \n",
    "\n",
    "            lData = dNodeData['values']\n",
    "            lNodes.append(sNode)\n",
    "            dNodePowers[sNode] = lData\n",
    "\n",
    "            for lDataPoint in lData:\n",
    "                iTick = lDataPoint[0]\n",
    "                iPower = lDataPoint[1]\n",
    "                lTicks.append(iTick)\n",
    "                dPowerData[(iTick, sNode)] = iPower\n",
    "        \n",
    "        lTicks.sort()\n",
    "        setTicksOrdered = set(lTicks)\n",
    "\n",
    "        # Below we create the structure of the DataFrame\n",
    "        dfJobPower = pd.DataFrame(\n",
    "            index = setTicksOrdered,\n",
    "            columns = lNodes\n",
    "        )\n",
    "\n",
    "        # We now populate the DataFrame with the power data. \n",
    "        for tIndex in dPowerData.keys():\n",
    "            dfJobPower.loc[tIndex[0], tIndex[1]] = dPowerData[tIndex]\n",
    "\n",
    "        # We now change the format of the timestamp\n",
    "        dfJobPower.index = pd.to_datetime(dfJobPower.index, unit='s', utc=True)\n",
    "        dfJobPower['Date'] = dfJobPower.index.strftime('%Y-%m-%d')\n",
    "        dfJobPower['Date'] = dfJobPower['Date'].str.cat((((dfJobPower.index).hour * 2) + ((dfJobPower.index).minute//30) + 1).astype(str), sep=\" \")\n",
    "\n",
    "        # We now resample the DataFrame and interpolate the data to obtain \n",
    "        # a datapoint for each 30s. \n",
    "        dfJobPower[dfJobPower.columns[:-1]] = dfJobPower[dfJobPower.columns[:-1]].apply(pd.to_numeric, axis=1)\n",
    "        dfJobPower = dfJobPower.resample('30S', origin='start').interpolate()\n",
    "\n",
    "        return dfJobPower\n",
    "    else:\n",
    "        # We will now create a DataFrame containing the power data\n",
    "        dNodePowers = {}\n",
    "        dPowerData = {}\n",
    "        lTicks = []\n",
    "        lNodes = []\n",
    "\n",
    "        lData = []\n",
    "\n",
    "        for tTick in list(range(1690355610, 1690360500, 30)):\n",
    "            if tTick < (1690355610 + 1800):\n",
    "                lData.append([tTick, 300])\n",
    "            elif (1690355610 + 1800) < tTick and tTick < (1690355610 + 3089):\n",
    "                lData.append([tTick, 150])\n",
    "            elif tTick > (1690355610 + 3089):\n",
    "                lData.append([tTick, 300])\n",
    "\n",
    "        sNode = 'TEST'\n",
    "\n",
    "        lNodes.append(sNode)\n",
    "        dNodePowers[sNode] = lData\n",
    "\n",
    "        for lDataPoint in lData:\n",
    "            iTick = lDataPoint[0]\n",
    "            iPower = lDataPoint[1]\n",
    "            lTicks.append(iTick)\n",
    "            dPowerData[(iTick, sNode)] = iPower\n",
    "        \n",
    "        lTicks.sort()\n",
    "        setTicksOrdered = set(lTicks)\n",
    "\n",
    "        # Below we create the structure of the DataFrame\n",
    "        dfJobPower = pd.DataFrame(\n",
    "            index = setTicksOrdered,\n",
    "            columns = lNodes\n",
    "        )\n",
    "\n",
    "        # We now populate the DataFrame with the power data. \n",
    "        for tIndex in dPowerData.keys():\n",
    "            dfJobPower.loc[tIndex[0], tIndex[1]] = dPowerData[tIndex]\n",
    "\n",
    "        # We now change the format of the timestamp\n",
    "        dfJobPower.index = pd.to_datetime(dfJobPower.index, unit='s', utc=True)\n",
    "\n",
    "        # We now resample the DataFrame and interpolate the data to obtain \n",
    "        # a datapoint for each 30s. \n",
    "        dfJobPower = dfJobPower.apply(pd.to_numeric, axis=1)\n",
    "        dfJobPower = dfJobPower.resample('30S').interpolate()\n",
    "\n",
    "        # We will now create the 'Date' Column\n",
    "        dfJobPower['Date'] = dfJobPower.index.strftime('%Y-%m-%d')\n",
    "        dfJobPower['Date'] = dfJobPower['Date'].str.cat((((dfJobPower.index).hour * 2) + ((dfJobPower.index).minute//30) + 1).astype(str), sep=\" \")\n",
    "\n",
    "        return dfJobPower\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da535f3",
   "metadata": {},
   "source": [
    "We will now run our code on the test data to ensure it works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5199f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total energy usage for the test job is: 351.875 Wh\n",
      "The calculated energy is 99% of the expected energy.\n"
     ]
    }
   ],
   "source": [
    "iTestEnergy = sum(getJobEnergy(getJobPower(0, dfTest))['TEST'])\n",
    "iCorrectTestEnergy = 353.7\n",
    "\n",
    "print(f'The total energy usage for the test job is: {iTestEnergy} Wh')\n",
    "\n",
    "iEnergyPercentage = iTestEnergy/iCorrectTestEnergy * 100\n",
    "\n",
    "print(f'The calculated energy is {round(iEnergyPercentage)}% of the expected energy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1288787",
   "metadata": {},
   "source": [
    "## Calculating the Carbon Footprint of the Jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113812b",
   "metadata": {},
   "source": [
    "We first need to add the UTC columns to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0803ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSacctExtended = dfToUTC(dfSacctExtended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aea73d",
   "metadata": {},
   "source": [
    "Now that we know that our code that calculates the carbon footprint works, we will apply this function to all of the jobs in our job DataFrame.\n",
    "\n",
    "We will use the jobLib library to parallelise this code and maximise its efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7671976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first create a list of DataFrames, each one containing jobs that \n",
    "# run on a different partition.\n",
    "\n",
    "lPartitionDFs = []\n",
    "\n",
    "for partition in dfSacctExtended['Partition'].unique():\n",
    "    bPartitionMask = dfSacctExtended['Partition'] == partition\n",
    "    lPartitionDFs.append(dfSacctExtended[bPartitionMask])\n",
    "\n",
    "# We will also create a list of DataFrames, each one containing jobs that \n",
    "# run for a different user.\n",
    "\n",
    "lUserDFs = []\n",
    "\n",
    "for user in dfSacctExtended['User'].unique():\n",
    "    bUserMask = dfSacctExtended['User'] == user\n",
    "    lUserDFs.append(dfSacctExtended[bUserMask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCarbonEnergy(df):\n",
    "    df[['CarbonFootprint(gCO2)', 'TotalEnergy', 'EquivalentDistance(km)']] = df.apply(lambda row : getCarbonFootprint(row.name, df)[0], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddf055",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/?token=150cfc129b995f56723f5eb13787bcf6dbf2ed56dc4af617'. Verify the server is running and reachable. (request to http://localhost:8888/hub/api failed, reason: connect ECONNREFUSED 127.0.0.1:8888). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "lCarbonDFs = Parallel(n_jobs=8)(delayed(findCarbonEnergy)(df) for df in lUserDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e3914",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/?token=150cfc129b995f56723f5eb13787bcf6dbf2ed56dc4af617'. Verify the server is running and reachable. (request to http://localhost:8888/hub/api failed, reason: connect ECONNREFUSED 127.0.0.1:8888). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dfFinal = pd.concat(lCarbonDFs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
